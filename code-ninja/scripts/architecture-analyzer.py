#!/usr/bin/env python3
"""
Architecture Analyzer
ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Python-Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°, Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹,
Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹, ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸.

ĞŸÑ€Ğ¸Ğ¼ĞµÑ€:
    python architecture-analyzer.py /path/to/project
"""

import ast
import os
import sys
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict
import json


class CodeMetrics:
    """ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ¾Ğ´Ğ°"""
    def __init__(self):
        self.lines_of_code = 0
        self.functions = 0
        self.classes = 0
        self.imports = 0
        self.complexity = 0  # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ


class ArchitectureAnalyzer:
    """ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°"""
    
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.files_analyzed = 0
        self.issues = []
        self.metrics = defaultdict(CodeMetrics)
        self.import_graph = defaultdict(set)  # Ñ„Ğ°Ğ¹Ğ» -> Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹
        self.all_files = []
        
    def analyze(self) -> Dict:
        """Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·"""
        print(f"ğŸ” ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒÑ {self.project_path}...")
        
        # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ²ÑĞµ Python Ñ„Ğ°Ğ¹Ğ»Ñ‹
        self.all_files = list(self.project_path.rglob("*.py"))
        self.files_analyzed = len(self.all_files)
        
        print(f"   ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {self.files_analyzed} Python Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²")
        
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ»
        for file_path in self.all_files:
            self._analyze_file(file_path)
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹
        self._check_circular_imports()
        self._check_large_files()
        self._check_long_functions()
        self._check_architecture_smells()
        
        return self._generate_report()
    
    def _analyze_file(self, file_path: Path):
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ğ´Ğ¸Ğ½ Ñ„Ğ°Ğ¹Ğ»"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
        except Exception as e:
            self.issues.append({
                'type': 'read_error',
                'file': str(file_path),
                'message': f'ĞĞµ Ğ¼Ğ¾Ğ³Ñƒ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ»: {e}'
            })
            return
        
        # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
        rel_path = str(file_path.relative_to(self.project_path))
        self.metrics[rel_path].lines_of_code = len(lines)
        
        # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ AST
        try:
            tree = ast.parse(content)
        except SyntaxError as e:
            self.issues.append({
                'type': 'syntax_error',
                'file': rel_path,
                'message': f'Ğ¡Ğ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}'
            })
            return
        
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ AST
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                self.metrics[rel_path].functions += 1
                # Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ (ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹)
                self.metrics[rel_path].complexity += self._count_branches(node)
            elif isinstance(node, ast.ClassDef):
                self.metrics[rel_path].classes += 1
            elif isinstance(node, (ast.Import, ast.ImportFrom)):
                self.metrics[rel_path].imports += 1
                self._record_imports(rel_path, node)
    
    def _count_branches(self, node: ast.FunctionDef) -> int:
        """Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²ĞµÑ‚Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ğ² Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸"""
        branches = 1  # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For)):
                branches += 1
            elif isinstance(child, ast.ExceptHandler):
                branches += 1
        return branches
    
    def _record_imports(self, file_path: str, node):
        """Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ñ„Ğ°"""
        if isinstance(node, ast.Import):
            for alias in node.names:
                self.import_graph[file_path].add(alias.name.split('.')[0])
        elif isinstance(node, ast.ImportFrom) and node.module:
            module = node.module.split('.')[0]
            self.import_graph[file_path].add(module)
    
    def _check_circular_imports(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹"""
        # Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ Ğ³Ñ€Ğ°Ñ„ Ğ¸ Ğ¸Ñ‰ĞµĞ¼ Ñ†Ğ¸ĞºĞ»Ñ‹
        visited = set()
        recursion_stack = set()
        
        def has_cycle(node, path=[]):
            visited.add(node)
            recursion_stack.add(node)
            path.append(node)
            
            for neighbor in self.import_graph.get(node, set()):
                if neighbor not in visited:
                    if has_cycle(neighbor, path.copy()):
                        return True
                elif neighbor in recursion_stack:
                    # ĞĞ°ÑˆĞ»Ğ¸ Ñ†Ğ¸ĞºĞ»
                    cycle_start = path.index(neighbor)
                    cycle = path[cycle_start:] + [neighbor]
                    self.issues.append({
                        'type': 'circular_import',
                        'severity': 'high',
                        'message': f'Ğ¦Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚: {" -> ".join(cycle)}'
                    })
                    return True
            
            recursion_stack.remove(node)
            return False
        
        for file in self.import_graph:
            if file not in visited:
                has_cycle(file)
    
    def _check_large_files(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹"""
        LARGE_FILE_THRESHOLD = 500  # ÑÑ‚Ñ€Ğ¾Ğº
        VERY_LARGE_THRESHOLD = 1000
        
        for file_path, metrics in self.metrics.items():
            if metrics.lines_of_code > VERY_LARGE_THRESHOLD:
                self.issues.append({
                    'type': 'very_large_file',
                    'severity': 'high',
                    'file': file_path,
                    'message': f'Ğ¤Ğ°Ğ¹Ğ» ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹: {metrics.lines_of_code} ÑÑ‚Ñ€Ğ¾Ğº (>{VERY_LARGE_THRESHOLD})',
                    'suggestion': 'Ğ Ğ°Ğ·Ğ±ĞµĞ¹Ñ‚Ğµ Ñ„Ğ°Ğ¹Ğ» Ğ½Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸'
                })
            elif metrics.lines_of_code > LARGE_FILE_THRESHOLD:
                self.issues.append({
                    'type': 'large_file',
                    'severity': 'medium',
                    'file': file_path,
                    'message': f'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ„Ğ°Ğ¹Ğ»: {metrics.lines_of_code} ÑÑ‚Ñ€Ğ¾Ğº (>{LARGE_FILE_THRESHOLD})',
                    'suggestion': 'Ğ Ğ°ÑÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸'
                })
    
    def _check_long_functions(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ (Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸)"""
        HIGH_COMPLEXITY = 10
        VERY_HIGH_COMPLEXITY = 20
        
        # Ğ­Ñ‚Ğ¾ ÑƒĞ¿Ñ€Ğ¾Ñ‰Ñ‘Ğ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° - Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° Ğ½ÑƒĞ¶ĞµĞ½ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·
        for file_path, metrics in self.metrics.items():
            if metrics.complexity > VERY_HIGH_COMPLEXITY:
                self.issues.append({
                    'type': 'very_complex_file',
                    'severity': 'high',
                    'file': file_path,
                    'message': f'Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ñ†Ğ¸ĞºĞ»Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ: {metrics.complexity}',
                    'suggestion': 'Ğ ĞµÑ„Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¸Ñ‚Ğµ: Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸Ñ‚Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, ÑƒĞ¿Ñ€Ğ¾ÑÑ‚Ğ¸Ñ‚Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ'
                })
    
    def _check_architecture_smells(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹"""
        # Ğ˜Ñ‰ĞµĞ¼ God Classes (Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ ĞºĞ»Ğ°ÑÑĞµ)
        # Ğ˜Ñ‰ĞµĞ¼ utils.py (ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¸Ğ¼ĞµĞ½Ğ°)
        
        for file_path in self.metrics.keys():
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ¸Ğ¼ĞµĞ½Ğ°
            filename = Path(file_path).name
            if filename in ['utils.py', 'helpers.py', 'common.py', 'misc.py']:
                self.issues.append({
                    'type': 'vague_name',
                    'severity': 'low',
                    'file': file_path,
                    'message': f'Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¾Ğ±Ñ‰ĞµĞµ Ğ¸Ğ¼Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°: {filename}',
                    'suggestion': 'ĞŸĞµÑ€ĞµĞ¸Ğ¼ĞµĞ½ÑƒĞ¹Ñ‚Ğµ Ğ² something_specific.py'
                })
    
    def _generate_report(self) -> Dict:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚"""
        total_lines = sum(m.lines_of_code for m in self.metrics.values())
        total_functions = sum(m.functions for m in self.metrics.values())
        total_classes = sum(m.classes for m in self.metrics.values())
        
        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ğ¾ severity
        severity_order = {'high': 0, 'medium': 1, 'low': 2}
        sorted_issues = sorted(
            self.issues,
            key=lambda x: severity_order.get(x.get('severity', 'low'), 3)
        )
        
        report = {
            'summary': {
                'files_analyzed': self.files_analyzed,
                'total_lines': total_lines,
                'total_functions': total_functions,
                'total_classes': total_classes,
                'issues_found': len(self.issues),
                'high_severity': len([i for i in self.issues if i.get('severity') == 'high']),
                'medium_severity': len([i for i in self.issues if i.get('severity') == 'medium']),
                'low_severity': len([i for i in self.issues if i.get('severity') == 'low']),
            },
            'top_files_by_size': sorted(
                [(f, m.lines_of_code) for f, m in self.metrics.items()],
                key=lambda x: x[1],
                reverse=True
            )[:10],
            'issues': sorted_issues[:20],  # Ğ¢Ğ¾Ğ¿ 20 Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼
            'recommendations': self._generate_recommendations()
        }
        
        return report
    
    def _generate_recommendations(self) -> List[str]:
        """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸"""
        recs = []
        
        high_issues = [i for i in self.issues if i.get('severity') == 'high']
        if high_issues:
            recs.append(f"ğŸš¨ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²ÑŒÑ‚Ğµ {len(high_issues)} ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´ Ñ€ĞµĞ»Ğ¸Ğ·Ğ¾Ğ¼")
        
        circular = [i for i in self.issues if i['type'] == 'circular_import']
        if circular:
            recs.append("ğŸ”„ Ğ£ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‚Ğµ Ñ†Ğ¸ĞºĞ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ñ‹ - Ğ¾Ğ½Ğ¸ Ğ²Ñ‹Ğ·Ñ‹Ğ²Ğ°ÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ")
        
        large_files = [i for i in self.issues if i['type'] in ['large_file', 'very_large_file']]
        if large_files:
            recs.append(f"ğŸ“¦ Ğ Ğ°Ğ·Ğ±ĞµĞ¹Ñ‚Ğµ {len(large_files)} Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ½Ğ° Ğ¼Ğ¾Ğ´ÑƒĞ»Ğ¸")
        
        if not recs:
            recs.append("âœ… ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ²Ñ‹Ğ³Ğ»ÑĞ´Ğ¸Ñ‚ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾! ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°Ğ¹Ñ‚Ğµ Ğ² Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ´ÑƒÑ…Ğµ")
        
        return recs


def print_report(report: Dict):
    """ĞšÑ€Ğ°ÑĞ¸Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°"""
    print("\n" + "="*60)
    print("ğŸ“Š ĞĞ¢Ğ§ĞĞ¢ ĞĞĞĞ›Ğ˜Ğ—Ğ ĞĞ Ğ¥Ğ˜Ğ¢Ğ•ĞšĞ¢Ğ£Ğ Ğ«")
    print("="*60)
    
    summary = report['summary']
    print(f"\nğŸ“ Ğ¤Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: {summary['files_analyzed']}")
    print(f"ğŸ“ Ğ’ÑĞµĞ³Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº ĞºĞ¾Ğ´Ğ°: {summary['total_lines']:,}")
    print(f"âš¡ Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¹: {summary['total_functions']}")
    print(f"ğŸ”· ĞšĞ»Ğ°ÑÑĞ¾Ğ²: {summary['total_classes']}")
    
    print(f"\nğŸ¯ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾: {summary['issues_found']}")
    if summary['high_severity']:
        print(f"   ğŸ”´ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…: {summary['high_severity']}")
    if summary['medium_severity']:
        print(f"   ğŸŸ¡ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ñ…: {summary['medium_severity']}")
    if summary['low_severity']:
        print(f"   ğŸŸ¢ ĞĞ¸Ğ·ĞºĞ¸Ñ…: {summary['low_severity']}")
    
    if report['top_files_by_size']:
        print("\nğŸ“ Ğ¡Ğ°Ğ¼Ñ‹Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹:")
        for file, lines in report['top_files_by_size'][:5]:
            print(f"   {lines:>4} ÑÑ‚Ñ€Ğ¾Ğº  {file}")
    
    if report['issues']:
        print("\nâš ï¸  Ğ¢ĞĞŸ ĞŸĞ ĞĞ‘Ğ›Ğ•Ğœ:")
        for i, issue in enumerate(report['issues'][:10], 1):
            severity = issue.get('severity', 'low')
            icon = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(severity, 'âšª')
            print(f"\n   {icon} [{severity.upper()}] {issue.get('type', 'issue')}")
            if 'file' in issue:
                print(f"      Ğ¤Ğ°Ğ¹Ğ»: {issue['file']}")
            print(f"      {issue['message']}")
            if 'suggestion' in issue:
                print(f"      ğŸ’¡ {issue['suggestion']}")
    
    print("\nğŸ’¡ Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ˜:")
    for rec in report['recommendations']:
        print(f"   {rec}")
    
    print("\n" + "="*60)


def main():
    if len(sys.argv) < 2:
        print("Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ: python architecture-analyzer.py <Ğ¿ÑƒÑ‚ÑŒ_Ğº_Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñƒ>")
        print("ĞŸÑ€Ğ¸Ğ¼ĞµÑ€: python architecture-analyzer.py /path/to/ai-router")
        sys.exit(1)
    
    project_path = sys.argv[1]
    
    if not os.path.exists(project_path):
        print(f"âŒ ĞŸÑƒÑ‚ÑŒ Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚: {project_path}")
        sys.exit(1)
    
    analyzer = ArchitectureAnalyzer(project_path)
    report = analyzer.analyze()
    
    print_report(report)
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ JSON Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
    output_file = "architecture_report.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {output_file}")
    
    # Exit code Ğ´Ğ»Ñ CI/CD
    high_issues = report['summary']['high_severity']
    if high_issues > 0:
        print(f"\nâš ï¸  ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {high_issues} ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼!")
        sys.exit(1)


if __name__ == "__main__":
    main()
