#!/usr/bin/env python3
"""
Dynamic Parser
–ü–∞—Ä—Å–∏–Ω–≥ JavaScript-—Ä–µ–Ω–¥–µ—Ä–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é Playwright

–ü—Ä–∏–º–µ—Ä:
    python dynamic-parser.py --url "https://example.com" --wait-for "#content"
    python dynamic-parser.py --url "https://example.com" --selector ".price" --screenshot
"""

import argparse
import asyncio
import json
import re
from typing import List, Optional, Dict
from urllib.parse import urljoin, urlparse

# –õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ Playwright
try:
    from playwright.async_api import async_playwright, Page, Browser
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False


class DynamicParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤"""
    
    def __init__(self, headless: bool = True, proxy: Optional[str] = None):
        self.headless = headless
        self.proxy = proxy
        self.browser: Optional[Browser] = None
        self.context = None
    
    async def __aenter__(self):
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError("Playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏: pip install playwright && playwright install chromium")
        
        self.playwright = await async_playwright().start()
        
        browser_options = {"headless": self.headless}
        if self.proxy:
            browser_options["proxy"] = {"server": self.proxy}
        
        self.browser = await self.playwright.chromium.launch(**browser_options)
        self.context = await self.browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={"width": 1920, "height": 1080}
        )
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
    
    async def parse_page(
        self,
        url: str,
        wait_for: Optional[str] = None,
        selector: Optional[str] = None,
        screenshot: bool = False,
        timeout: int = 30
    ) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        page: Page = await self.context.new_page()
        
        try:
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            print(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞: {url}")
            response = await page.goto(url, wait_until="networkidle", timeout=timeout * 1000)
            
            if not response:
                raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            
            status = response.status
            print(f"üìä –°—Ç–∞—Ç—É—Å: {status}")
            
            # –ñ–¥—ë–º —ç–ª–µ–º–µ–Ω—Ç –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if wait_for:
                print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞: {wait_for}")
                try:
                    await page.wait_for_selector(wait_for, timeout=timeout * 1000)
                except Exception as e:
                    print(f"‚ö†Ô∏è –≠–ª–µ–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≥—Ä—É–∑–∫–∏ JS
            await page.wait_for_timeout(2000)
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            title = await page.title()
            content = await page.content()
            
            # –°–∫—Ä–∏–Ω—à–æ—Ç
            screenshot_path = None
            if screenshot:
                screenshot_path = f"screenshot_{hash(url)}.png"
                await page.screenshot(path=screenshot_path, full_page=True)
                print(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {screenshot_path}")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É
            extracted_data = None
            if selector:
                print(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {selector}")
                elements = await page.query_selector_all(selector)
                extracted_data = []
                
                for el in elements[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–º–∏ 10
                    text = await el.inner_text()
                    href = await el.get_attribute("href")
                    src = await el.get_attribute("src")
                    
                    extracted_data.append({
                        "text": text.strip() if text else None,
                        "href": urljoin(url, href) if href else None,
                        "src": urljoin(url, src) if src else None
                    })
                
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(extracted_data)}")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
            links = await self._extract_links(page, url)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤
            meta = await self._extract_meta(page)
            
            return {
                "url": url,
                "status": status,
                "title": title,
                "content_length": len(content),
                "screenshot": screenshot_path,
                "extracted_data": extracted_data,
                "links": links[:20],  # –ü–µ—Ä–≤—ã–µ 20 —Å—Å—ã–ª–æ–∫
                "meta": meta
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return {
                "url": url,
                "error": str(e)
            }
        finally:
            await page.close()
    
    async def _extract_links(self, page: Page, base_url: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫"""
        links = await page.query_selector_all("a[href]")
        result = []
        
        for link in links:
            href = await link.get_attribute("href")
            text = await link.inner_text()
            
            if href:
                absolute_url = urljoin(base_url, href)
                result.append({
                    "url": absolute_url,
                    "text": text.strip()[:100] if text else "",
                    "is_external": urlparse(absolute_url).netloc != urlparse(base_url).netloc
                })
        
        return result
    
    async def _extract_meta(self, page: Page) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤"""
        meta_selectors = [
            "meta[name='description']",
            "meta[property='og:title']",
            "meta[property='og:description']",
            "meta[property='og:image']",
            "meta[name='keywords']"
        ]
        
        meta = {}
        for selector in meta_selectors:
            try:
                el = await page.query_selector(selector)
                if el:
                    name = await el.get_attribute("name") or await el.get_attribute("property")
                    content = await el.get_attribute("content")
                    if name and content:
                        meta[name] = content
            except:
                pass
        
        return meta
    
    async def monitor_changes(
        self,
        url: str,
        selector: str,
        interval: int = 60,
        callback = None
    ):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        previous_value = None
        
        while True:
            try:
                result = await self.parse_page(url, selector=selector)
                current_data = result.get("extracted_data", [])
                current_value = json.dumps(current_data, sort_keys=True) if current_data else ""
                
                if previous_value is not None and current_value != previous_value:
                    print(f"üîÑ –ò–∑–º–µ–Ω–µ–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–∞ {url}!")
                    if callback:
                        await callback(result)
                
                previous_value = current_value
                print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —Å–ª–µ–¥—É—é—â–∞—è —á–µ—Ä–µ–∑ {interval}—Å")
                await asyncio.sleep(interval)
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")
                await asyncio.sleep(interval)


async def main():
    parser = argparse.ArgumentParser(description='Dynamic Parser - –ø–∞—Ä—Å–∏–Ω–≥ JS-—Å–∞–π—Ç–æ–≤')
    parser.add_argument('--url', '-u', required=True, help='URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞')
    parser.add_argument('--wait-for', '-w', help='CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –æ–∂–∏–¥–∞–Ω–∏—è')
    parser.add_argument('--selector', '-s', help='CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--screenshot', action='store_true', help='–°–¥–µ–ª–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç')
    parser.add_argument('--timeout', '-t', type=int, default=30, help='–¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö')
    parser.add_argument('--headless', action='store_true', default=True, help='Headless —Ä–µ–∂–∏–º')
    parser.add_argument('--proxy', '-p', help='–ü—Ä–æ–∫—Å–∏ (http://host:port)')
    parser.add_argument('--output', '-o', help='–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON')
    parser.add_argument('--monitor', '-m', type=int, help='–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞–∂–¥—ã–µ N —Å–µ–∫—É–Ω–¥')
    
    args = parser.parse_args()
    
    async with DynamicParser(headless=args.headless, proxy=args.proxy) as parser:
        if args.monitor:
            print(f"üîç –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞–∂–¥—ã–µ {args.monitor} —Å–µ–∫—É–Ω–¥...")
            print("–ù–∞–∂–º–∏ Ctrl+C –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
            try:
                await parser.monitor_changes(args.url, args.selector, args.monitor)
            except KeyboardInterrupt:
                print("\n‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        else:
            result = await parser.parse_page(
                url=args.url,
                wait_for=args.wait_for,
                selector=args.selector,
                screenshot=args.screenshot,
                timeout=args.timeout
            )
            
            # –í—ã–≤–æ–¥
            print("\n" + "=" * 50)
            print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢:")
            print("=" * 50)
            print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {result.get('title')}")
            print(f"–°—Ç–∞—Ç—É—Å: {result.get('status')}")
            print(f"–î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {result.get('content_length', 0)} chars")
            
            if result.get('meta'):
                print(f"\nüìù –ú–µ—Ç–∞-—Ç–µ–≥–∏:")
                for key, value in result['meta'].items():
                    print(f"  {key}: {value[:100]}...")
            
            if result.get('extracted_data'):
                print(f"\nüîç –ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ ({len(result['extracted_data'])} —ç–ª–µ–º–µ–Ω—Ç–æ–≤):")
                for i, item in enumerate(result['extracted_data'][:5], 1):
                    print(f"  {i}. {item.get('text', 'N/A')[:80]}...")
            
            if result.get('screenshot'):
                print(f"\nüì∏ –°–∫—Ä–∏–Ω—à–æ—Ç: {result['screenshot']}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=2, ensure_ascii=False)
                print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {args.output}")


if __name__ == "__main__":
    if not PLAYWRIGHT_AVAILABLE:
        print("‚ùå Playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏: pip install playwright && playwright install chromium")
        exit(1)
    
    asyncio.run(main())
