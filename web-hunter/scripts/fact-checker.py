#!/usr/bin/env python3
"""
Fact Checker
–ü–µ—Ä–µ–∫—Ä—ë—Å—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

–ü—Ä–∏–º–µ—Ä:
    python fact-checker.py --claim "NVIDIA bought Groq for $20B"
    python fact-checker.py --claim "OpenAI released GPT-5" --min-confidence 0.8
"""

import argparse
import asyncio
import json
import re
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import aiohttp


@dataclass
class FactCheckResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–∞"""
    claim: str
    verdict: str  # "true", "false", "partially_true", "unverified"
    confidence: float  # 0.0 - 1.0
    sources: List[Dict]
    contradictions: List[Dict]
    explanation: str
    checked_at: datetime


class FactChecker:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –≤ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
    
    # –ê–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–¥–æ–º–µ–Ω—ã)
    TRUSTED_SOURCES = {
        "reuters.com", "bloomberg.com", "ft.com", "wsj.com",
        "techcrunch.com", "theverge.com", "wired.com",
        "arxiv.org", "nature.com", "science.org",
        "official": ["openai.com", "anthropic.com", "google.com", "microsoft.com"]
    }
    
    # –°–ª–æ–≤–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    SUSPICIOUS_WORDS = ["viral", "shocking", "you won't believe", "doctors hate", "secret"]
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def search_for_claim(self, claim: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Ñ–∞–∫—Ç–∞"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Brave Search API –∏–ª–∏ fallback –Ω–∞ Reddit/HN
        results = []
        
        # –ü–æ–∏—Å–∫ –≤ Reddit
        try:
            reddit_results = await self._search_reddit(claim)
            results.extend(reddit_results)
        except Exception as e:
            print(f"Reddit search error: {e}")
        
        # –ü–æ–∏—Å–∫ –≤ HackerNews
        try:
            hn_results = await self._search_hackernews(claim)
            results.extend(hn_results)
        except Exception as e:
            print(f"HN search error: {e}")
        
        return results
    
    async def _search_reddit(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ Reddit"""
        results = []
        
        try:
            url = "https://www.reddit.com/search.json"
            params = {"q": query, "sort": "relevance", "limit": 10}
            
            async with self.session.get(url, params=params, headers={
                "User-Agent": "Web-Hunter Bot 1.0"
            }) as response:
                if response.status == 200:
                    data = await response.json()
                    posts = data.get("data", {}).get("children", [])
                    
                    for post in posts:
                        post_data = post.get("data", {})
                        results.append({
                            "title": post_data.get("title", ""),
                            "text": post_data.get("selftext", ""),
                            "url": f"https://reddit.com{post_data.get('permalink', '')}",
                            "source": "reddit",
                            "score": post_data.get("score", 0),
                            "created": datetime.fromtimestamp(post_data.get("created_utc", 0))
                        })
        except Exception as e:
            print(f"Reddit error: {e}")
        
        return results
    
    async def _search_hackernews(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ HackerNews"""
        results = []
        
        try:
            url = "https://hn.algolia.com/api/v1/search"
            params = {"query": query, "tags": "story"}
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    hits = data.get("hits", [])
                    
                    for hit in hits:
                        results.append({
                            "title": hit.get("title", ""),
                            "text": hit.get("story_text", ""),
                            "url": hit.get("url") or f"https://news.ycombinator.com/item?id={hit.get('objectID')}",
                            "source": "hackernews",
                            "score": hit.get("points", 0),
                            "created": datetime.fromtimestamp(hit.get("created_at_i", 0))
                        })
        except Exception as e:
            print(f"HN error: {e}")
        
        return results
    
    def analyze_sentiment(self, texts: List[str], claim: str) -> Tuple[str, float]:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–ª–∏ –æ–ø—Ä–æ–≤–µ—Ä–∂–µ–Ω–∏—è —Ñ–∞–∫—Ç–∞"""
        claim_keywords = set(claim.lower().split())
        
        confirm_signals = 0
        deny_signals = 0
        total_mentions = 0
        
        confirm_words = ["confirmed", "true", "yes", "indeed", "announced", "official"]
        deny_words = ["false", "fake", "rumor", "not true", "denied", "debunked"]
        
        for text in texts:
            text_lower = text.lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            if any(kw in text_lower for kw in claim_keywords):
                total_mentions += 1
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
                if any(word in text_lower for word in confirm_words):
                    confirm_signals += 2
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø—Ä–æ–≤–µ—Ä–∂–µ–Ω–∏–µ
                if any(word in text_lower for word in deny_words):
                    deny_signals += 2
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                if any(word in text_lower for word in self.SUSPICIOUS_WORDS):
                    deny_signals += 1
        
        if total_mentions == 0:
            return "unverified", 0.0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–µ—Ä–¥–∏–∫—Ç
        if confirm_signals > deny_signals * 2:
            confidence = min(confirm_signals / total_mentions, 1.0)
            return "true", confidence
        elif deny_signals > confirm_signals * 2:
            confidence = min(deny_signals / total_mentions, 1.0)
            return "false", confidence
        elif confirm_signals > 0 or deny_signals > 0:
            return "partially_true", 0.5
        else:
            return "unverified", 0.3
    
    def find_contradictions(self, sources: List[Dict], claim: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"""
        contradictions = []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —Ç–æ–Ω—É (–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç/–æ–ø—Ä–æ–≤–µ—Ä–≥–∞—é—Ç)
        confirm_sources = []
        deny_sources = []
        
        for source in sources:
            text = f"{source.get('title', '')} {source.get('text', '')}".lower()
            
            if any(word in text for word in ["confirmed", "true", "announced", "official"]):
                confirm_sources.append(source)
            elif any(word in text for word in ["false", "fake", "denied", "debunked"]):
                deny_sources.append(source)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∏ —Ç–µ –∏ –¥—Ä—É–≥–∏–µ ‚Äî —ç—Ç–æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ
        if confirm_sources and deny_sources:
            contradictions.append({
                "type": "conflicting_reports",
                "confirm_count": len(confirm_sources),
                "deny_count": len(deny_sources),
                "sample_confirm": confirm_sources[0] if confirm_sources else None,
                "sample_deny": deny_sources[0] if deny_sources else None
            })
        
        return contradictions
    
    def generate_explanation(self, verdict: str, confidence: float, sources: List[Dict], contradictions: List[Dict]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –≤–µ—Ä–¥–∏–∫—Ç–∞"""
        if verdict == "true":
            if confidence > 0.8:
                return f"–§–∞–∫—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.0%}). –ù–∞–π–¥–µ–Ω—ã –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞—è–≤–ª–µ–Ω–∏—è –∏–ª–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ –Ω–∞–¥—ë–∂–Ω—ã—Ö –°–ú–ò."
            else:
                return f"–§–∞–∫—Ç, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –≤–µ—Ä–µ–Ω, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.0%}). –ï—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö, –Ω–æ –Ω–µ –≤—Å–µ –æ–Ω–∏ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã."
        
        elif verdict == "false":
            if confidence > 0.8:
                return f"–§–∞–∫—Ç –æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç. –ù–∞–π–¥–µ–Ω—ã –ø—Ä—è–º—ã–µ –æ–ø—Ä–æ–≤–µ—Ä–∂–µ–Ω–∏—è –æ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–ª–∏ —Ñ–∞–∫—Ç—á–µ–∫–∏–Ω–≥–æ–≤—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.0%})."
            else:
                return f"–§–∞–∫—Ç, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –ª–æ–∂–µ–Ω (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.0%}). –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ñ–µ–π–∫–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
        
        elif verdict == "partially_true":
            return f"–§–∞–∫—Ç —á–∞—Å—Ç–∏—á–Ω–æ –≤–µ—Ä–µ–Ω –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.0%}). –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—ë—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
        
        else:  # unverified
            return f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–∞ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.0%}). –ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö."
    
    async def check(self, claim: str, min_confidence: float = 0.7) -> FactCheckResult:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–∞"""
        print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞: {claim}")
        print("-" * 50)
        
        # –ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
        sources = await self.search_for_claim(claim)
        print(f"üìö –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)}")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        texts = [f"{s.get('title', '')} {s.get('text', '')}" for s in sources]
        verdict, confidence = self.analyze_sentiment(texts, claim)
        print(f"üìä –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –≤–µ—Ä–¥–∏–∫—Ç: {verdict} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
        
        # –ü–æ–∏—Å–∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
        contradictions = self.find_contradictions(sources, claim)
        if contradictions:
            print(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è: {len(contradictions)}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
        explanation = self.generate_explanation(verdict, confidence, sources, contradictions)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if confidence < min_confidence and verdict in ["true", "false"]:
            verdict = "unverified"
            explanation += f" –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ({confidence:.0%}) –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ ({min_confidence:.0%})."
        
        return FactCheckResult(
            claim=claim,
            verdict=verdict,
            confidence=confidence,
            sources=sources[:5],  # –¢–æ–ø-5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            contradictions=contradictions,
            explanation=explanation,
            checked_at=datetime.now()
        )
    
    def format_result(self, result: FactCheckResult, format_type: str = "text") -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        
        verdict_emoji = {
            "true": "‚úÖ",
            "false": "‚ùå",
            "partially_true": "‚ö†Ô∏è",
            "unverified": "‚ùì"
        }.get(result.verdict, "‚ùì")
        
        if format_type == "json":
            return json.dumps({
                "claim": result.claim,
                "verdict": result.verdict,
                "confidence": result.confidence,
                "explanation": result.explanation,
                "sources_count": len(result.sources),
                "contradictions": len(result.contradictions),
                "checked_at": result.checked_at.isoformat()
            }, indent=2, ensure_ascii=False)
        
        elif format_type == "markdown":
            lines = [
                f"# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–∞\n",
                f"**–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ:** {result.claim}\n",
                f"**–í–µ—Ä–¥–∏–∫—Ç:** {verdict_emoji} {result.verdict.upper()}\n",
                f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {result.confidence:.0%}\n",
                f"**–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ:** {result.checked_at.strftime('%Y-%m-%d %H:%M')}\n",
                f"\n## –û–±—ä—è—Å–Ω–µ–Ω–∏–µ\n",
                f"{result.explanation}\n",
                f"\n## –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ({len(result.sources)})\n"
            ]
            
            for i, source in enumerate(result.sources, 1):
                lines.append(f"{i}. [{source.get('title', 'N/A')[:60]}...]({source.get('url', '')})")
            
            if result.contradictions:
                lines.append(f"\n## –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è\n")
                for c in result.contradictions:
                    lines.append(f"- {c['type']}: {c.get('confirm_count', 0)} –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç vs {c.get('deny_count', 0)} –æ–ø—Ä–æ–≤–µ—Ä–≥–∞—é—Ç")
            
            return "\n".join(lines)
        
        else:  # text
            lines = [
                f"{verdict_emoji} –í–ï–†–î–ò–ö–¢: {result.verdict.upper()}",
                f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence:.0%}",
                f"üìù –û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {result.explanation}",
                f"",
                f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ({len(result.sources)}):"
            ]
            
            for source in result.sources:
                lines.append(f"  ‚Ä¢ {source.get('title', 'N/A')[:70]}")
                lines.append(f"    {source.get('url', '')}")
            
            return "\n".join(lines)


async def main():
    parser = argparse.ArgumentParser(description='Fact Checker - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤')
    parser.add_argument('--claim', '-c', required=True, help='–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏')
    parser.add_argument('--min-confidence', '-m', type=float, default=0.7,
                       help='–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0.0-1.0)')
    parser.add_argument('--output', '-o', choices=['text', 'json', 'markdown'],
                       default='text', help='–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞')
    parser.add_argument('--save', '-s', help='–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª')
    
    args = parser.parse_args()
    
    async with FactChecker() as checker:
        result = await checker.check(args.claim, args.min_confidence)
        output = checker.format_result(result, args.output)
        
        print("\n" + "=" * 60)
        print(output)
        print("=" * 60)
        
        if args.save:
            with open(args.save, 'w', encoding='utf-8') as f:
                f.write(output)
            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {args.save}")


if __name__ == "__main__":
    asyncio.run(main())
